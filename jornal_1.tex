
%
\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}




% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi




% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/



% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{graphicx}
\usepackage{subfloat}
\usepackage{subfigure}
\usepackage{subfig}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{CJK}
\usepackage{amsmath}
\usepackage{caption}





\usepackage[font=small,labelsep=period]{caption}
\captionsetup{%
  figurename=Fig.,
  tablename=Table
  }

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{Task-Oriented Visual Servo System of Robot Arm for 3D Object based on Automatic Multilayer Networks Learning Approach }




\author{\IEEEauthorblockN{Ren C. Luo\IEEEauthorrefmark{1},
and Po-Yu Chuang\IEEEauthorrefmark{2},
}

\IEEEauthorblockA{\IEEEauthorrefmark{1} \IEEEauthorrefmark{2}Electrical Engineering Department, National
Taiwan University, Taipei, Taiwan 106}

}


\IEEEtitleabstractindextext{%
\begin{abstract}
Visual servo systems are widely applied on industrial robot arm recent years. A visual servo system could provide additional perception of robot arm, and make it more robust in different applications. In this paper, we propose an intelligent visual servo system which could automatically learn the model of unknown 3D objects, and self-supervised the results. Unlike traditional vision-based learning approaches, proposed learning system is task-oriented which means learning approaches only concern the relation between input and target rather then each individual part. To construct connections between input and target,the learning approach is established based on multi-layers networks. Multi-layers networks is used to model necessary knowledge in different domains.  The experiment results show the feasibility of proposed structure for self-taught, and multi-layers networks could effectively transfer knowledge between different domains.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Visual servo system, Multi-layers network, Machine learning
\end{IEEEkeywords}}



% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.







% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}


% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{R}{obot} arm with visual servo system had been widely applied in automatic industrial production line recent years[1-4]. Visual servo system provides additional perception of environment, and make robot arm become more adaptive to complete various tasks. For vision system, model-based recognition methods are commonly used in industrial application. The performance is mainly related to the labeled data which is captured manually. Hence, the system is hard to adapt with adding new kinds of work pieces. For 2D object recognition, users need to capture numerous raw data of target objects in specific poses. These cumbersome works lift to much more complex level while the methods are expanded to 3D object recognition. These manual works not only increase the labor cost, but also point out the dilemma of present vision-based robot which is unable to automatically adapt to various assignments. 

In general industrial purposes, we do not really concern about all details of entire 3D object. Instead, the relation between input and target is the key point for completing the tasks like pick and place,etc. To solve these issues, we propose an intelligent tasked-oriented visual servo based system which acquires ability of learning relational model of input and target automatically. In task-oriented view, system tends to learn the relation between input and target rather than delicate models for target 3D objects. Therefore, the state problem is that We only provide labeled data which face of 3D objects are desired to be placed on top by robot arm, but the other faces of 3D objects are unknown. The labeled data is considered as target face, and the input faces are arbitrary objects with random faces on top. The input of system is 2-D image data, and output is rotation angle of robot arm in Cartesian space which are different feature domains. Therefore, the traditional single layer model[HMM,SVM, GMM] which end in a linear or kernel classifier is not enough. We introduced a multilayer model to tackle our problems. 

The learning of multilayer model achieve dramatically success recent years while deep learning architectures showed up. Hinton et al. [deep learning algorithm] proposed a hierarchical structure which hidden layers are formed by lower level feature to higher level, and had been successfully applied on different research fields[Deep learning application]. Comparing with traditional \textbf{ANN} model[Ref. of neural network], the deep learning method is aim to learn the representation of data in different level rather than produces classifiers through features in the same level. Enlightening by deep learning methods, hierarchical structure is applied to our model which is constructed by four layers: \textbf{Feature} (image based), \textbf{Descriptor}, \textbf{Object} and \textbf{RotationAngle} (for robot arm).

Through this model, the feature in different domains could be correlated through hierarchical structure, but the system still can not automatically learn the relation between input images and output rotation angle. Being a self-taught system, the ability which could "infer" latent edges between labeled and unlabeled data in is needed. Latent edge means two variables in different layers exist an edge in graph model if prior data is sufficient, but, in our case, system only have small amount of prior data. Hence, there are many latent edges which are waiting to be revealed through learning process. 

The most challenge part of state problem is that the appearance of different faces of a single object might be huge different, so we design three modules to tackle self-taught problem. Firstly, we design a probabilistic based image descriptor. Extracting scale- and rotation-invariant sparse feature is a pervasive topic in areas of computer vision. Although many brilliant methods[10-14] provide high quality performance by extracting sparse features, the sparse feature is not compact on inferring the relational model. The sparse feature only model strong features of observed face shows on top, but most of faces is unknown in our cases. we need a descriptor which can provide sufficient information for inferring latent edges, but still retain scale- and rotation-invariant. Proposed probabilistic based descriptor is established based on the Markov Logic Network (MLN) [15]. MLN is an approach combines first-order logic and probabilistic graphical model. First-order logic enable compactly representing the neighborhood of feature points. Probabilistic graphical model can reveal latent edges by proper inference method, and also handle the uncertainty. 

Secondly, transfer information module is proposed for constructing latent edge. Transfer information module is realized by Self-taught Clustering algorithm [refer.]. Self-taught Clustering algorithm is a transfer learning method [6-9....need more] which is built for enhancing model through large amount of auxiliary unlabeled data. The input face can be considered auxiliary unlabeled data, and find co-cluster between priors face in the dataset. Hereafter, we further utilize the distribution of co-cluster to infer the possible rotation angle for robot arm, and robot arm would rotate the object from input face to output face by rotating inferred angle. Finally, the validation module is an eye-to-hand camera which used to validate the error between the output face and desired target face. Hereafter, the validation module feedback the error to the model to refined the model. Through these three module, proposed system can automatically learn the relation between input image and output rotation angle with only labeled the target face of each object.

In this paper, we start with briefly overview of system design and structure in section 2. The MLN-based descriptor for recognized object is described in section 3. Section 4 introduces how to model the proposed multilayer networks, build up and refine the structure of database for matching. Then, we compare the performance of proposed system with several different features and descriptors in section 5. Finally, the performance review and conclusion are presented in final section.

\begin{figure}[!t]
\begin{center}
%\subfigure[System architecture]{}
\includegraphics[width=0.5\textwidth]{j_img/fig1.jpg}
\caption{System architecture}\label{test}
\end{center}
\end{figure}

\begin{figure*}[!t]
\begin{center}
\includegraphics*[width=7 in]{j_img/fig3.jpg}
\caption{Hierarchical-deep model for self-taught system}\label{test}
\end{center}
\end{figure*}


%\hfill mds
\section{System architecture} 

The main purpose of this system is that desire to automatically derive the relationship between input face and target face of 3D assigned objects. The only prior knowledge are the target face. Input is arbitrary assigned object with random face on top, so it is very likely a unknown face of assigned object rather than prior target face. Therefore, system has to infer the correlation between input and existed prior target face in the pool.  Proposed system is shown in Fig. 1. camera 1 captures images of all input objects with random faces on top, and 
construct MLN-based descriptor for each input. Then, system match the input with data in Database and output rotation angle for robot arm. After robot arm placing a input object, camera 2 would validate result, and feedback error for refining existed model. The system architecture in Fig. 1 is realized by a hierarchical-deep model in Fig. 2. 

The variables in the same layer are independent, and vertical adjacent two layers are full connected. Variables in \textbf{Feature($\bf{\Gamma^{C}}$)} layer are extracted image features, and variables in both \textbf{Classified Descriptor($\bf{D^C}$} and \textbf{Unclassified Descriptor($\bf{D^U}$)} are MLN-based descriptor. Variables in \textbf{Rotation angle($\bf{\Theta^{C}}$)} and \textbf{Inferred Rotation angle($\bf{\Theta^U}$)} is rotation angle (Row ($\alpha$), Pitch($\beta$) , Yaw($\gamma$)) respect to target faces. Finally, variables in \textbf{Object($\bf{O^C}$)} is combination of rotation angle.


The difference between classic \textbf{Deep Belief Networks}(\textbf{DBN})[Ref.] is that proposed model exist two parallel parts as shown in Fig. 3. \textbf{$\bf{D^C}$-$\bf{\Theta^{C}}$} and \textbf{$\bf{D^U}$-$\bf{\Theta^U}$} have no connection between each other, but both have full connection with deepest layer $\bf{O^C}$ and first layer \textbf{$\bf{\Gamma^{C}}$}. To handle tons of unknown data, the structure of connection will dynamically change with observed evidences. Sparse coding method is used to constructs edge in the model, most of connection is zero which is called latent edge in this paper. Latent edge might become non-zero while some new evidence has been discovered. For variable $d^C_w$ in layer $\bf{D^C}$, the sparse coding result should be formulated as:

\begin{equation}
d^C_w=\sum_{i\in{d^C_w}}a_{i}\Gamma_{i}+\sum_{j\notin{d^C_w}}b_{j}\Gamma_{j}
\end{equation}

Although Eq.(1) can handle the problem of latent edge, it's impractical to sample all possible conditions whenever new evidence showing up. Therefore, proposed model separate descriptor layer into two parallel parts as: 

\begin{equation}
d^C_w=\sum_{\Gamma_{i}\in{d^C_w}}a_{i}\Gamma_{i}
\end{equation}
\begin{equation}
d^U_r=\sum_{\Gamma_{j}\in{d^U_r}}b_{j}\Gamma_{j}
\end{equation}

$d^C_w$ is considered a prior descriptor which the edge between $\bf{\Theta^{C}}$ and $\bf{O^C}$ had been established. Therefore, the left part of parallel layers can be considered as static model until there is a query classified to the $\bf{D^U}$. $\bf{D^U}$ layer is the set of descriptors which we haven't known that these descriptors are corespondent to which object. Therefore, we propose a inference method to infer the possible rotation angle, and camera 2 will checks the results. If inference is success, variable $d^U_r$ and $\theta^{inference}$ become new variables of \textbf{Classified Descriptor($\bf{D^C}$)-Rotation angle($\bf{\Theta^{C}}$)}. Meanwhile, new edges of left part of parallel layers are established which can be considered latent edges become visible. By transfer variable between two parallel parts, variables in $\bf{D^C}$ will dynamically grow with number of inputs. 

\begin{figure}[!t]
\begin{center}
\subfigure[Result of background subtracting and clustering]{\includegraphics[width=2.5in]{j_img/fig2b.jpg}}
\subfigure[Serial captured frames]{\includegraphics[width=3.5in]{j_img/fig2a.jpg}}
\caption{Preprocessing of input objects}\label{test}
\end{center}
\end{figure}


\section{MLN-based Descriptor}
\subsection{The concept of constructing MLN-based descriptor}
Being an self-taught system, deriving more valuable information from raw data helps system deriving more reliable results with poor prior knowledge. Most of present image descriptors [10-14] are constructed based on strong sparse feature point, because these points are consistent even in different environment. These kinds of descriptor can efficiently and precisely match given image. Nevertheless, most of observed face is not in prior data, so we need a descriptor which can infer the relation between observations and priors. To avoid lose of information, we choose normal distributed feature instead of sparse feature. Since different faces of an object may exist different strong features, normal distributed feature is more suit for our case. 

For prepossessing of input images, each channel of RGB domain is classified into 5 parts, and get 125 classes in RGB domain. An input image will be segmented by these classes. In Fig. 3(a), an observed face of input object is segmented into 4 classes, and class 0 is background. Hereafter, predicates for MLN networks are constructed by segmented results. We have only two kinds of predicate \textit{ne(a,v)} and  \textit{des(x)} for MLN model. Variable \textit{a} is an atom cluster, and variable \textit{v} is a neighbor of atom cluster. Variable \textit{x} is a MLN-based descriptor. The variables of feature layer in Fig. 2 are predicates \textit{ne(a,v)}. Since every classes can be the atom cluster, we have $\binom{125}{2}$ binary variables in feature layer.

Taking Fig. 3(a) as an example, the predicates of preprocessed image are shown in Table 1, and first order logic is formulated as:

\begin{equation}
\forall{a}\forall{v}\quad\textit{ne(a,v)}\Rightarrow\textit{des(x)}
\end{equation}

Each image will further be down sampled, and derived several images with different scales. For each image, we derive  \textbf{F*S} formulas where F is number of serial captured images and S is number of images with different scales. Through these formulas, a MLN model can be constructed. The probability distribution over possible world $d^in$ specified by MLN is given by:

\begin{displaymath}
P(\bf{D^{in}} = d^{in}) = \frac{1}{Z}exp(\sum_{j=1}^{F*S} w_j n_j(d^{in}))
\end{displaymath}
\begin{equation}
\bf{Z}=\sum_{d^{in}\in\bf{D^{in}}}exp(\sum_{j=1}^{F*S} w_j n_j(d^{in}))
\end{equation}

Where  $n_j(d^{in})$ is the number of true grounding of formula j in $d^{in}$, and $w_j$ is weight of formula j .

Consequently, probability distribution Eq.(5) is a MLN-based descriptor for 2D face recognition. 

\begin{table}[!t]
\caption{Example of predicates and first-order logic formulas}
\centering
\begin{tabular}{|c|c|c|c|c|} 
\hline
Key Atom & 1 & 2 & 3 & 4\\
\hline\hline
\multirow{4}{*}{Predicates} & 
ne(1,2) & 
ne(2,1) & 
ne(3,1) &
ne(4,1)\\
\cline{2-5}
 &
ne(1,3) &
ne(2,3) &
ne(3,2) &
ne(4,2) \\
\cline{2-5}
 &
ne(1,4) &
ne(2,4) &
 &
 \\
\cline{2-5}
 &
ne(1,0) &
 &
 &
 \\
\hline
\end{tabular} 
\end{table} 

The complexity of formulas constructing process depend on the number of different kinds of segmentation. From table 1, there are some equivalent predicates (e.g. $\textit{ne(1,2)}$ and $\textit{ne(2,1)}$) which might be repeated sampled. To reduce the complexity, we use dynamic programming algorithm in algorithm 1 to enhance efficiency. The algorithm can avoid repeated sampling. 



\begin{algorithm}
  \caption{Algorithm for sampling neighbours of key atoms}
   $\bf{Function}$ Sampling$(S$\_$img,S,S',S^*)$\\ 
  $\bf{Input:}$\\
   $S$\_$img$, segmented input image\\
   $S[ns,p,l]$, the contour point $p$ of $ns^{th}$ cluster with label $l$\\
   $S'$, contour pixel of each cluster\\
   $S^*[ns,p^*,l^*]$, neighbour pixel $p^*$ of $S.p$ with label $l^*$\\ 
  $\bf{Output:}$\\
  $neighbour[n,ln,la]$, the neighbour with label $ln$ of $n^{th}$ key atom with label  $la$         
  \begin{algorithmic}[1]
    \State $\bf{Random}$(Point in $S$\_$img$)
    \For{k$\leftarrow$ 1 to number of cluster}
    
      \State $n\leftarrow k$
      \State $S'\leftarrow\emptyset$
     \For{i$\leftarrow$1 to size of contour}
     
      \State $S'\leftarrow$ contour pixel of label $ln$
      \State $S^*.p^*\leftarrow$ neighbour of $S'$ with different label
      \If{$S^*.label$ is changed}
      	\State $n\leftarrow n+1$
      	\State $neighbour[n,ln,la]\leftarrow$
      	\State $\qquad\qquad neighbour[n,S^*.l^*,S.l]$
      	\EndIf 
      	\For{$j\leftarrow\ k\ to\ n-1$}
      	\State $neighbour[j,ln,la]\leftarrow $
      	\State $\qquad\qquad neighbour[j,ln,S^*.l^*]$
      	\EndFor
      \EndFor
     \State $S[ns,p,l]\leftarrow S[k,S',S.l]$
     \State $S^*\leftarrow S^*-S$  
    \EndFor\\
    
	\State $\bf{Random}$ $(S^*)$
	\State $\bf{Until}$ $S^*=\emptyset$
	\State $\bf{Return}$ $neighbour[n,ln,la]$
    
  \end{algorithmic}
\end{algorithm}

\subsection{Inference and Weight learning of MLN-based descriptor}
The weights of MLN-based descriptor is learned by maximizing the pseudo-log-likelihood. Since each descriptor can be considered as a closed world, we only need to consider the atoms which derive from captured serial frames. Comparing with uniform sampling approach, maximizing pseudo-log-likelihood is more efficient, because pseudo-log likelihood only need to considered relational data. The pseudo-log -likelihood of eq.(5) can be written as:

\begin{equation}
\log P^*_w(\mathbf{D^{in}} = d^{in}) = \sum_{j=1}^{F*S} \log P_{w}(\mathbf{D^{in}} = d^{in}|\mathbf{MB}(d^{in}))
\end{equation}

Where $\bf{MB}(d^{in})$ is Markov blanket while $d^{in}$ is observed. The MLN weights are learned generatively by maximizing the pseudo-log-likelihood of Markov blanket. The gradient of the pseudo-log-likelihood with respect to the weight is:

\begin{displaymath}
\begin{array}{ll}
\dfrac{\partial}{\partial w_i}\log P^*_w(\mathbf{D^{in}} = d^{in})= &\\\\
\sum^{F*S}_{j=1}\{n_i(d^{in})-P_w(\mathbf{D^{in}}=0|\mathbf{MB}(d^{in}))n_i(d^{in}=0) &\\
\end{array}
\end{displaymath}
\begin{equation}
-P_w(\mathbf{D^{in}}=1|\mathbf{MB}(d^{in})n_i(d^{in}=1)\}
\end{equation}

Where $n_i(d^{in}=0)$ is the number of true grounding of $j^{th}$ formula while force $\mathbf{d^{in}}=0$, and similar for $n_i(d^{in}=1)$. The learning of pseudo-log-likelihood in our approach are further boosted by the L-BFGS optimizer [24], to make entire process become more efficiency.



\subsection{Matching of MLN-based descriptors}
For each constructed input descriptor $d^{in}_k$, system would search for the matched descriptor in the database, and further arrange it to the proper layer of $\bf{D^C}$ or $\bf{D^U}$ as shown in fig.2. Since input is possible to be assigned to one of parallel layers, matching step is separated into two parts. One is utilized pseudo-log-likelihood to decide that observation should be assigned to which layer. The pseudo-log-likelihood of descriptors matching could be formulated as:


\begin{equation}
\begin{array}{ll}

\mathbf{argMax}P(\mathbf{D^C} = d^C_w  | \mathbf{D^{in}} = d^{in} , \mathbf{\Gamma}_{k \in {d^{in}}}) &\\\\
= \mathbf{argMax}P(d^{in} | \mathbf{MB}(d^{in}){P(d^{C}_w | \mathbf{MB}(d^{in}))}
\end{array}
\end{equation}


If input descriptor doesn't match any descriptor in $\bf{D^C}$ layer, the descriptor become a variable of  $\bf{D^U}$ layer. For a variable in $\bf{D^U}$, we would like to infer rotation angle to make input object can be placed on corresponding target face. Since the rotation angles for descriptors in $\bf{D^U}$ had been identified, the second part for matching is try to find a descriptor in $\bf{D^C}$ which have max co-cluster with input descriptor. Finding max co-cluster can be alternately considered as minimizing loss of information as:
\begin{equation}
\mathbf{argMin}(I(d^in , \mathbf{\Gamma}_{k \in d^{in} \cap d^C_w}) - I(d^C_w , \mathbf{\Gamma}_{k \in d^{in} \cap d^C_w}))
\end{equation}

Consequently, the common feature $\mathbf{\Gamma}_{k \in d^{in} \cap d^C_w}$ is further represented by co-Markov Blanket of $d^{in}$ and $d^C_w$, and the loss of mutual information can be further formulated by KL divergence [Ref.] as:

\begin{equation}
\begin{array}{ll}
\mathbf{argMin}D(P(d^{in} , \mathbf{MB}(d^{in},d^c_w))||P(d^c_w , \mathbf{MB}(d^{in},d^c_w)))\\\\
= \mathbf{argMin}\sum_{\Gamma_k\in\mathbf{MB}(d^{in},d^c_w)}P(\Gamma_k)D(P(d^{in}|\Gamma_k)||P(d^C_w|\Gamma_k))\\\\
\end{array}
\end{equation}

By Eq.(10), $d^C_w$ with min DL-divergence is considered acquired max co-cluster with $d^{in}$. The relation between the co-cluster become the evidence for inferring rotation angle of $d^in$. Through Eq.(8) and Eq.(10), the input descriptors had been classified to corresponding layer, and become inputs of $\bf{\Theta^{C}}$ or $\bf{\Theta^{U}}$ layer.


\section{Inference and Learning of Mapping between MLN-based Descriptor and Rotation Angle}
Since MLN-based descriptors are matched according to the neighborhoods of clusters, the descriptor is scale and pose invariant. To make robot arm place input objects to corresponding target pose, the relation between descriptor and rotation angle have to be constructed, and make knowledge could be transferred between different domains in proposed multilayer network. We assume different faces of same object include at least one common predicates, and the common predicates can be used to infer the relation between input and target pose. Set of rotation angle $\Theta$ is composited by $\theta_R$, $\theta_P$, and $\theta_Y$ which represent roll, pitch and yaw angle of 3-DOF end effector of robot arm. $\Theta$ is unknown at first, because there is  no prior knowledge of rotation angle for proposed system as mention before. $\Theta$ can be only predict by the common predicates between descriptors. For two matched descriptor, the common predicates has significant possibility to be the same parts of object, so the relation between common predicates in Cartesian space can be used to predicate possible rotation angle, and make inferred results reliable and accurate in several iteration. The mass center of each cluster is considered the position of each cluster in Cartesian space, and the center of images is origin of coordinate. Firstly, we sample the center atoms of common predicates between input and target descriptor, and compare the difference of position between each center atom. The differences are represent by vector which is formulated as:
\begin{equation}
\vec{V_c}=\vec{V^T_c}-\vec{V^{in}_c}
\end{equation}
Where $\vec{V_c}$ is the vector of key atom c. $\vec{V^T_c}$ and $\vec{V^{in}_c}$ are the position vector of key atom c of target and input descriptor. According to the MLN-based descriptors, the formula with higher weighting means more reliable. Reminding the example in table 1, every predicates of a formula belong to the same key atom, so the weight of each formulas could be further considered as the reliability of each center atom. Hence, we choice key atom which is included in the formula with the highest weight firstly, and transfer to rotation angle for robot arm. The result would be checked by camera 2. While the number of inputs grows, the results would become the set of vector $\boldsymbol{\vec{V}_{mb}}$ which means the set of vector of descriptor b in object m. The set of vector $\boldsymbol{\vec{V}_{mb}}$ is used to build up a Markov network model which could refine the predicating result based on the historical results which are identified by camera 2. Since the uncertainty of probabilistic descriptor that the matched input descriptor for same target descriptor might not be totally same one, we would like to build up a transfer function which can predict ideal rotation angle depend on different input descriptors. There are two factors have to be concerned: (1) the distribution of historical vector $\boldsymbol{\vec{V}_{mb}}$. (2) likelihood of input and target descriptor. Hence, the transfer function could be formulated by joint distribution:
\begin{equation}
\begin{array}{l}
P(L(d^{in}_{kT}|d^T_m),\vec{\mathbf{V}}_{\bf{mb}})=\\\\
\frac{1}{Z_V}exp(\sum_k\sum_b\lambda_t\mathbf{F}\{L(d^{in}_k|d^T_m)=l(d^{in}_kt|d^T_{mt}),\vec{\mathbf{V}}_{\bf{mb}}=\vec{v}_b\mathbf\})\\
\end{array}
\end{equation}

Where $L(d^{in}_{kT}|d^T_m)$ is a set of likelihood of input descriptor k and target descriptor during a period of time T. $l(d^{in}_kt|d^T_{mt})$ is the likelihood at time t. $\mathbf{F}\{*\}$ is feature function which is 1 while $*$ is true, and 0 otherwise. $\lambda_t$ is weight if feature function. This transfer function represents the mapping between set of input descriptors and the same target descriptor $d^T_m$.  While derive a new input descriptor, the predicated result can be derived by:

\begin{equation}
\begin{array}{l}
\mathbf{argMax}P(\vec{V^*}_{\vec{V}-\vec{V}_{fail}})=\\
\qquad\qquad\qquad\qquad P(\vec{V}^*|l(d^{in}_t|d^T_m),L(d^{in}_{t-1}|d^T_m),\mathbf{\vec{V}_{mb}})\\
\end{array}
\end{equation}

\section{Experiments}
The inputs of proposed system are serial images of each object, so most of open source databases can not be applied for proposed system, and also not suit for purposes in this paper. Therefore, the experiments are implemented through our own dataset. The testing dataset is constructed by twenty different kinds of chosen work pieces as shown in table 2.
The experiment is implemented based on several assumptions: The input objects are not occluded, and not adjacent with each other. The input objects are placed on conveyor with random poses, and we assume the probability of every faces showing on top is uniform distribution. 


\begin{table*}[!t]
\caption{Three classes of testing work pieces for experiments}
\begin{center}
\begin{tabular}{|c|c||c|c||c|c|}
\hline
\multicolumn{2}{|c||}{\bf{WP1}} & \multicolumn{2}{c||}{\bf{WP2}} & \multicolumn{2}{c|}{\bf{WP3}}\\
\hline\hline
WP1.1 & WP1.5 & WP 2.1 & WP 2.5 & WP3.1 & WP3.5\\
\hline
\includegraphics[width=1in,height=0.6in]{j_img/wp11.jpg} & 
\includegraphics[width=1in,height=0.6in]{j_img/wp15.jpg} & 
\includegraphics[width=1in,height=0.6in]{j_img/wp21.jpg} & 
\includegraphics[width=1in,height=0.6in]{j_img/wp25.jpg} &
\includegraphics[width=1in,height=0.6in]{j_img/wp31.jpg} & 
\includegraphics[width=1in,height=0.6in]{j_img/wp35.jpg} \\
\hline
WP1.2 & WP1.6 & WP 2.2 & WP 2.6 & WP3.2 & WP3.6\\
\hline
\includegraphics[width=1in,height=0.6in]{j_img/wp12.jpg} & 
\includegraphics[width=1in,height=0.6in]{j_img/wp16.jpg} & 
\includegraphics[width=1in,height=0.6in]{j_img/wp22.jpg} & 
\includegraphics[width=1in,height=0.6in]{j_img/wp26.jpg} &
\includegraphics[width=1in,height=0.6in]{j_img/wp32.jpg} & 
\includegraphics[width=1in,height=0.6in]{j_img/wp36.jpg} 
\\
\hline
WP1.3 & WP1.7 & WP 2.3 & & WP3.3 & WP3.7\\
\hline
\includegraphics[width=1in,height=0.6in]{j_img/wp13.jpg} & 
\includegraphics[width=1in,height=0.6in]{j_img/wp17.jpg} & 
\includegraphics[width=1in,height=0.6in]{j_img/wp23.jpg} & 
 &
\includegraphics[width=1in,height=0.6in]{j_img/wp33.jpg} & 
\includegraphics[width=1in,height=0.6in]{j_img/wp37.jpg} 
\\
\hline
WP1.4 & & WP 2.4 & & WP3.4 &\\
\hline
\includegraphics[width=1in,height=0.6in]{j_img/wp14.jpg} & 
 & 
\includegraphics[width=1in,height=0.6in]{j_img/wp24.jpg} & 
 &
\includegraphics[width=1in,height=0.6in]{j_img/wp34.jpg} & 
\\ 
\hline
\end{tabular}
\end{center}
\end{table*}

The testing work pieces are classified into three classes in table 2. For class $\bf{WP1}$, the work pieces are featureless and small, so it's hard to construct robustness descriptor even building relational model for entire model. For class $\bf{WP2}$, all work pieces acquire similar shapes or size, so, for general method, this kind of object is easily mismatch in the matching process. The work pieces in $\bf{WP3}$ are matched group of this experiment. The work pieces acquire sufficient feature for descriptor, and have plenty of information for identifying and constructing relational model. In the first stage of experiment, we would like to compare the performance of proposed system between different classes in different environments.  The results of different classes are shown in fig.6. In fig.6(b), the environment lighting is controlled by on-axis lighting source, so the information of object are more complete and distinct than images with lighting control in fig.6(a). The accuracy of recognized result is average of 100 times repeatedly testing.

 The system is considered convergence while accuracy is over 95$\%$, and stop learning approach. If the accuracy is under 95$\%$ again, the learning approach would be re-excuted. Comparing the results, in both cases, class $\bf{WP3}$ could be convergent with least input sample, and convergent time of class $\bf{WP2}$ is slowest. The results shows the efficiency of learning could be slightly improved by environment constrain, but the accuracy is not effected, and always over 95$\%$ after learning approach stopped. Similarly, twenty kinds of work pieces are included in learning stage in the same time, and the results are shown in fig. 7. The accuracy of each test is also the average of 100 times repeatedly testing. The results show that system need more inputs to convergent while more kinds of objects are included in learning stage. The performance is also slightly improved while environment is lighting controlled. In brief, these two experiments verify proposed system is competent to learn the relational model automatically. Although the learning rate would be dragged by the kinds of input objects, the learning rate still can be convergent by reasonable number of inputs. The result shows the system can be convergent by less than 1000 sample pieces with random poses. Furthermore, the accuracy of recognition is stable once learning approach completing, and would not be under threshold(95$\%$) again unless adding new kind input.


 
\begin{figure}[!t]
\centering
	\subfigure[Performance without environment constrains]{
		\includegraphics[width=3 in]{j_img/exp02.png}
	}

	\subfigure[Performance with environment constrains]{
		\includegraphics[width=3 in]{j_img/exp01.png}
	}

\caption{Experimental Results of different classes in different environment constrains}

\end{figure}

\begin{figure}[!t]
\centering
	\subfigure[Performance without environment constrains]{
		\includegraphics[width=3 in]{j_img/exp04.png}
	}

	\subfigure[Performance with environment constrains]{
		\includegraphics[width=3 in]{j_img/exp03.png}
	}

\caption{Experimental Results of all work pieces in different environment constrains}

\end{figure}

The experiment in fig. 6 and 7 testified the performance of proposed system could automatically learn the relational model of variant kinds of objects. Then, we would like to compare the performance of proposed system with other advanced approaches. Since none of similar systems could handle this issue in our survey, so the comparisons would be done by dividing our system into two parts. One is 2-D descriptors for each face of objects, and the other is machine learning approach for learning relational model.

For the descriptor part, four kinds of other descriptors are chosen to compare with proposed system. B-SIFT[25]and Edge-SIFT[26] are modified versions of SIFT approach which enhanced the accuracy of feature point registration. BRISK[13] descriptor is constructed based on binary robust invariant scalable key points, and Zernike Moment (ZM)[11] phase-based descriptor is a moment-based descriptor which use the phase information of signal. All of these descriptors are representative methods in relative field recent years, and had been testified by plenty of researchers. To compare the robustness and accuracy, the performance is testified by two conditions. One is relationship of each faces is prior of system, and the descriptors only provide information for object matching. The experiments are implemented by the same learning approach which proposed in previous section.The other is no prior for learning approach that information of descriptor need to use for inferring the relational model. The ZM descriptor have the best performance in the condition without prior, but accuracies of descriptors are close. In condition without prior, the MLN-based descriptor acquire best performance which testified MLN-based descriptor is suited for self-taught system. 

Hereafter, the performance of different learning methods should be further discussed. The learning approach in proposed system need to learn the distribution of different domains, so regular learning approaches is hard to applied on proposed structure directly. The transfer learning methods are famous for handling cross domains problem, so the other three kinds of transfer learning approaches: Locally Weighted Ensemble approach(LWE)[7], Transductive SVM(TSVM)[8], and Weighted Neural Network(WNN)[9] are chosen to compare with proposed method. Similarly, the experiments are divided into two parts as shown in table 4. The result shows LEW had the best accuracy in the condition with prior, and proposed learning approaches acquire greatest performance in condition without prior, but, in both two conditions, the performance between different methods are pretty close. It's seem the results are mainly effected by the performance of the descriptor. The performance of descriptor not only influence the result of 2-D image, but also the relational model of each faces, so the experiments are reasonable. Furthermore, the results showed proposed system acquire best performance in automatic learning part.  

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\begin{table}[!t]
\caption{Comparisons of system performance with different 2-D descriptors}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|} 
\hline
\multirow{2}{*}{} &
&
\multicolumn{5}{|c|}{\bf{Descriptor}} \\
\cline{3-7}
 &
 & 
\bf{\tabincell{l}{MLN-\\based}} &
\bf{B-SIFT} &
\bf{\tabincell{l}{Edge-\\SIFT}} &
\bf{BRISK} &
\bf{ZM} 
 \\
\hline
\multirow{4}{*}{\tabincell{l}{With\\prior}} &
WP1 &
0.9781&
0.9664&
0.8384&
0.9556&
0.9788
\\
\cline{2-7}
&
WP2 &
0.9630&
0.9766&
0.9233&
0.9676&
0.9523
\\
\cline{2-7}
&
WP3 &
0.9901&
0.9963&
0.9454&
0.9899&
0.9949
\\
\cline{2-7}
&
All &
0.9594&
0.8982&
0.8066&
0.9432&
0.9634
\\
\hline\hline
\multirow{4}{*}{\tabincell{l}{Without\\prior}} &
WP1 &
0.9611&
0.7688&
0.6544&
0.8103&
0.7787
\\
\cline{2-7}
&
WP2 &
0.9505&
0.7043&
0.7123&
0.7197&
0.7979
\\
\cline{2-7}
&
WP3 &
0.9718&
0.8044&
0.7963&
0.8243&
0.8231
\\
\cline{2-7}
&
All &
0.9543&
0.6431&
0.6144&
0.7741&
0.7670
\\
\hline

\end{tabular} 
\end{table} 

\begin{table}[!t]
\caption{Comparisons of different transfer learning approach}
\centering
\begin{tabular}{|l|l|l|l|l|l|} 
\hline
\multirow{2}{*}{} &
&
\multicolumn{4}{|c|}{\bf{Transfer learning approach}} \\
\cline{3-6}
 &
 & 
\bf{Proposed} &
\bf{LEW} &
\bf{TSVM} &
\bf{WNN} 
\\
\hline
\multirow{4}{*}{\tabincell{l}{With\\prior}} &
WP1 &
0.9781&
0.9802&
0.9511&
0.9513
\\
\cline{2-6}
&
WP2 &
0.9630&
0.9763&
0.9690&
0.9601
\\
\cline{2-6}
&
WP3 &
0.9901&
0.9899&
0.9799&
0.9684
\\
\cline{2-6}
&
All &
0.9594&
0.9677&
0.9567&
0.9541
\\
\hline\hline
\multirow{4}{*}{\tabincell{l}{Without\\prior}} &
WP1 &
0.9611&
0.9601&
0.9543&
0.9103
\\
\cline{2-6}
&
WP2 &
0.9505&
0.9543&
0.9558&
0.9197
\\
\cline{2-6}
&
WP3 &
0.9718&
0.9788&
0.9699&
0.9346
\\
\cline{2-6}
&
All &
0.9603&
0.9553&
0.9497&
0.9486
\\
\hline

\end{tabular} 
\end{table} 




\section{Conclusion}
The automatic learning approaches for visual-servo system is an important part in industrial application. In this work, we reverse the concept of traditional visual-servo system. The robustness of feature points and descriptor is not the thing which should be most concerned. Instead, the relational model between input and output is the most important. 

To learn the relationship between input and output, we proposed a system architecture which can automatic learning and self-supervised the performance of learning or inference results. Since the relationship between input and output is consisted by multiple different domains, the relational model is further segmented into three domain:2-D descriptor, 3-D object and Cartesian space of robot arm. Instead of modelling by three independent networks, these three domains are integrated into one multi-layers networks. Comparing with traditional multi-layers percetron, proposed multi-layer networks is not used to model a complex non-linear distribution, but model the transfer function between different layers. 
The knowledge in different domains could be transfer between different layers through transfer functions which are called relational models in this paper.  Furthermore, a MLN-based descriptor is further proposed to assist inference relationship of each 2-D descriptor while the relational model is unknown. The experiments result shows the system could automatic learning the relational model, and performance could compete with other excellent methods.  





.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi




\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
Torgny Brogrdh, \emph{"Present and future robot control developmentAn industrial perspective"},\hskip 1em plus
  0.5em minus 0.4em\relax Annual Reviews in Control, Volume 31, Issue 1, pp. 6979, 2007.

\bibitem{IEEEhowto:kopka} 
Ebrahim Mattar, \emph{"Robotics Arm Visual Servo: Estimation of Arm-Space Kinematics Relations with Epipolar Geometry, Robotic Systems - Applications, Control and Programming"},\hskip 1em plus
  0.5em minus 0.4em\relax Dr. Ashish Dutta (Ed.), ISBN: 978-953-307-941-7, InTech, DOI: 10.5772/25605.

\bibitem{IEEEhowto:kopka} 
So-Youn Park ,Yeoun-Jae Kim ,Ju-Jang Lee ,Byung Soo Kim, and Khalid A.Alsaif,\emph{"Controlling robot arm manipulator using image-based visual servoing without pre-depth information"},\hskip 1em plus 0.5em minus 0.4em\relax  37th IEEE Interantional Conferece on Industrial Electronics, pp.3157-3161,Nov. 2011.

\bibitem{IEEEhowto:kopka} 
K. Deguchi, H. Sakurai, and S. Ushida,\emph{"A Goal Oriented just-in-time visual servoing for ball catching robot arm},\hskip 1em plus 0.5em minus 0.4em\relax in Int. Conf. on Intelligent Robots and Systems, Sept. 2008, pp. 30343039.
Sa\v{s}o D\v{z}eroski\emph{"Multi-relational Data Mining: An Introduction"},\hskip 1em plus 0.5em minus 0.4em\relax SIGKDD Explore Newsletter, Volume 5, Issue 1, July 2003, pp.1-16.

\bibitem{IEEEhowto:kopka} 
Sinno Jialin Pan, and  Qiang Yang,\emph{"A Survey on Transfer Learning"},\hskip 1em plus 0.5em minus 0.4em\relax Knowledge and Data Engineering, IEEE Transactions on , vol.22, no.10, pp.1345,1359, Oct. 2010


\bibitem{IEEEhowto:kopka} 
T. Dietterich, L. Getoor, and K. Murphy,\emph{"Statistical Relational Learning and its Connections to Other Fields"},\hskip 1em plus 0.5em minus 0.4em\relax ICML-2004 Workshop on Statistical Relational Learning (SRL), Banff, Canada, July 2004.

\bibitem{IEEEhowto:kopka} 
Jing Gao and Wei Fan and Jing Jiang and Jiawei Han, \emph{"Knowledge Transfer via Multiple Model Local Structure Mapping"},\hskip 1em plus 0.5em minus 0.4em\relax in the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,pp. 283-291,New York, USA,2008

\bibitem{IEEEhowto:kopka} 
T. Joachims, \emph{"Making large-scale svm learning practical."},\hskip 1em plus 0.5em minus 0.4em\relax advances in kernel methods - support vector learning, MIT-Press, 1999.
D. Lowe, \emph{"Distinctive Image Features from Scale-Invariant Keypoints"},\hskip 1em plus 0.5em minus 0.4em\relax International Journal of Computer Vision 60(2), pp. 91110, 2004.

\bibitem{IEEEhowto:kopka} 
A. J. Carlson, C. M. Cumby, J. L. R. Nicholas D. Rizzolo,
and D. Roth,\emph{"Snow learning architecture"},\hskip 1em plus 0.5em minus 0.4em\relax Technical report UIUCDCS, 1999.

\bibitem{IEEEhowto:kopka} 
H. Bay, A. Ess, T. Tuytelaars, and L. Gool,\emph{"SURF: Speeded up robust features"},\hskip 1em plus 0.5em minus 0.4em\relax Comput. Vis. Image Understand., vol. 110, no. 3, pp. 346359, Mar. 2008.

\bibitem{IEEEhowto:kopka} 
Zen Chen and Shu-Kuo Sun,\emph{"A Zernike Moment Phase-Based Descriptor for Local
Image Representation and Matching"},\hskip 1em plus 0.5em minus 0.4em\relax IEEE Trans. Image
Process., vol. 19, no. 1,pp. 205219, Jan. 2010.

\bibitem{IEEEhowto:kopka} 
A. Alahi, R. Ortiz, and P. Vandergheynst,\emph{"Freak: Fast retina keypoint"},\hskip 1em plus 0.5em minus 0.4em\relax CVPR, 2012.

\bibitem{IEEEhowto:kopka} 
S. Leutenegger, M. Chli, and R. Siegwart,\emph{"Brisk: Binary Robust Invariant Scalable Keypoints"},\hskip 1em plus 0.5em minus 0.4em\relax International conference on Computer Vision, 2011.

\bibitem{IEEEhowto:kopka} 
 Vijay Chandrasekhar, Gabriel Takacs, David Chen, Sam S. Tsai, Jatinder Singh, and Bernd Girod,\emph{"Transform coding of image feature descriptors,"},\hskip 1em plus 0.5em minus 0.4em\relax SPIE 7257, Visual Communications and Image Processing, 2009.

\bibitem{IEEEhowto:kopka} 
Matthew Richardson and Pedro Domingos,\emph{"Markov logic networks,"},\hskip 1em plus 0.5em minus 0.4em\relax International Journal of Machine Learning, Volume 62, Issue 1-2,pp 107-136, Feb. 2006.

\bibitem{IEEEhowto:kopka} 
L. Mihalkova, T. Huynh, and R.J. Mooney,\emph{Mapping and
Revising Markov Logic Networks for Transfer Learning,},\hskip 1em plus 0.5em minus 0.4em\relax Proc. 22nd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial Intelligence, pp 608-614, July 2007.

\bibitem{IEEEhowto:kopka} 
Kok, Stanley and Domingos, Pedro,\emph{"Learning the Structure of Markov Logic Networks"},\hskip 1em plus 0.5em minus 0.4em\relax Proceedings of the 22Nd International Conference on Machine Learning, pp 441-448, Germany, 2005.

\bibitem{IEEEhowto:kopka} 
Parag Singla and Pedro Domingos,\emph{"Discriminative training of Markov logic networks"},\hskip 1em plus 0.5em minus 0.4em\relax Proceedings of the international Conf. on Artificial Intelligence, 2005.

\bibitem{IEEEhowto:kopka} 
J. Shi and J. Malik,\emph{"Normalized cuts and image segmentation"},\hskip 1em plus 0.5em minus 0.4em\relax Pattern Analysis and Machine Intelligence, IEEE Transactions on , vol.22, no.8, pp.888,905, Aug 2000.

\bibitem{IEEEhowto:kopka} 
Karthikeyan Vaiapury, Anil  Aksay and Ebroul Izquierdo,\emph{"GrabcutD: Improved Grabcut Using Depth Information"},\hskip 1em plus 0.5em minus 0.4em\relax Proceedings of the 2010 ACM Workshop on Surreal Media and Virtual Cloning, pp 57-62, New York, USA, 2010.

\bibitem{IEEEhowto:kopka} 
Z. Zivkovic,\emph{"Improved adaptive Gaussian mixture model for background subtraction"},\hskip 1em plus 0.5em minus 0.4em\relax Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International Conference on , vol.2, no., pp.28,31 Vol.2, 23-26 Aug. 2004.

\bibitem{IEEEhowto:kopka} 
G. Frahling and C. Sohler,\emph{"A fast k-means implementation using coresets"},\hskip 1em plus 0.5em minus 0.4em\relax  Proceedings of the twenty-second annual symposium on Computational geometry (SoCG),2006.

\bibitem{IEEEhowto:kopka} 
Tong Simon, and Daphne Koller,\emph{"Support vector machine active learning with applications to text classification"},\hskip 1em plus 0.5em minus 0.4em\relax  The Journal of Machine Learning Research 2 pp 45-66, 2002.

\bibitem{IEEEhowto:kopka} 
Fei Sha and Fernando Pereira,\emph{"Shallow parsing with conditional
random fields"},\hskip 1em plus 0.5em minus 0.4em\relax Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, Volume 1, 2003.

\bibitem{IEEEhowto:kopka} 
Yanning Zhang,Zhi-Hua Zhou, Changshui Zhang and Li, Ying,\emph{"B-SIFT: A Highly Efficient Binary SIFT Descriptor for Invariant Feature Correspondence"},\hskip 1em plus 0.5em minus 0.4em\relax Intelligent Science and Intelligent Data Engineering, pp 426-433, 2012

\bibitem{IEEEhowto:kopka} 
S. Zhang, Q. Tian, K. Lu, Q. Huang and W. Gao,\emph{"Edge-SIFT: Discriminative binary descriptor for scalable partial-duplicate mobile search"},\hskip 1em plus 0.5em minus 0.4em\relax  IEEE Trans. Image Process., vol. 22, no. 7, pp. 28892902, Jul. 2013.












\end{thebibliography}


\end{document}


