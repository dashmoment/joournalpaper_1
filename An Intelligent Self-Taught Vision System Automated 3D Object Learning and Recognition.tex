
\documentclass{tADR2e}

%\hyphenation{op-tical net-works semi-conduc-tor}
%\usepackage{graphicx}
%\usepackage{subfloat}
%\usepackage{subfigure}
%\usepackage{graphics}
%\usepackage{epsfig}
%\usepackage{CJK}
%\usepackage{amsmath}
%\usepackage{caption}
%\usepackage{subfig}
%\usepackage{color}
%\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{amssymb}
%\usepackage{amsmath}

%% Pusedo code 
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
%% Pusedo code %%


\begin{document}
\jvol{00} \jnum{00} \jyear{2013} \jmonth{January}

\title{ An Intelligent Self-Taught Vision System Automated 3D Object Learning and Recognition}

\author{Ren C. Luo$^{\ast}$\thanks{$^{\ast}$Corresponding author. Email: renluo@
ntu.edu.tw
\vspace{6pt}} and Po Yu Chuang$^{\dagger}$
\\\vspace{6pt}  $^{\ast,\dagger}${\em{Center for Intelligent Robotics and Automation Research, National Taiwan University, Taipei 10617, Taiwan}}\\\vspace{6pt}\received{v1.0 released January 2015}}

\maketitle

\begin{abstract}
Vision systems for 3D object recognition are widely applied on industrial integrating with robot arm. Conventionally, vision, learning approach, and robot arm are separated into three different systems, so that learning approach can only learn the distributions of input data, but cannot refine qualities of input and output without manual intervention. Therefore, it may cause misfiring performance by erratic input or sustainable growth database. In this paper, we propose an intelligent vision system for 3D object recognition which is able to automatically construct and refine model for 3D object recognition without any manual intervention. Although 2D images and rotation angles of robot arm are different domains, we model 3D objects by multiple 2D images and rotation angles, and information from different domains are integrated by a Hierarchical-Deep (HD) model with parallel branch. The model hierarchically extracts information by domains and levels. The parallel branch distinct labeled and unlabeled data to avoid performance drag by sustainable growth of unlabeled input. The relationships between label and unlabeled data are learned by proposed self-taught approach. The experimental results support the feasibility of proposed structure which can transfer knowledge in different domains with limited prior knowledge, and complete assigned task by only modeling the relation between input and output.\medskip

\begin{keywords}Automation; Computer vision; Image recognition; Learning systems; Intelligent robots 
\end{keywords}\medskip

\end{abstract}


\section{Introduction}

Robot arm with vision has been widely applied in automatic industrial production line for many years [1-4]. New hurdles of conventional vision system arise due to the rise of consumer electronic market. The components in assembling production line are become small volume with large varieties. The types of components are also changed rapidly because of short product life cycle. These conditions are tough for conventional vision system. Model-based recognition methods are commonly used in present industrial applications. The performance is mainly related to the manually labeled data. These manual works not only increase the labor cost, but also point out the dilemma of present vision-based robot is unable to automatically adapt to various assignments, and growing database.

Therefore, we propose an intelligent system which is able to handle sustainable growth of unlabeled input, and refine existed model without any manual intervention. The situation of this paper is that we only provide target face of 3D objects which are intended to be placed on top by robot arm, but the other faces of 3D objects are unknown. The only prior knowledge is target faces, and inputs are arbitrary objects with random faces on top. The input of system is 2D image, and output is rotation angle of robot arm in Cartesian. Since input and output are different domains, the traditional single layer models [5,6] which end in a linear or kernel classifier are not enough. We introduce a \textit{\textbf{Hierarchical-Deep(HD)}} model[7] to tackle the problems. 

The learning of HD model achieves dramatically success recent years. Hinton et al. [7] proposed deep structure learning which hidden layers are formed by lower level feature to higher level hierarchically, and had been successfully applied on different research fields [8-11]. The proposed HD model is constructed by four layers:\textbf{Feature}, \textbf{Descriptor}, \textbf{Object} and \textbf{RotationAngle}. Being an automatic system, the ability which could "infer" latent edges between labeled and unlabeled data is needed. Latent edge means two variables in different layers exist an edge in graph model if prior data is sufficient, but, in our case, system only have small amount of prior data. Hence, there are many latent edges which are waiting to be revealed through learning process. 

The most challenge part is that the appearance of different faces of a single object might be quite different, so we design three modules to tackle self-taught problem. Firstly, we design a probabilistic based image descriptor. Although many methods [12-16] provide high quality performance by extracting sparse features, the sparse feature is not compact on inferring latent edges. The sparse feature only model strong features of observed face, but most of faces is unknown in our case. We need a descriptor which can provide sufficient information for inference, but still retain scale- and rotation-invariant. Proposed probabilistic based descriptor is established based on the \textit{\textbf{Markov Logic Network (MLN)}} [17-20]. MLN is an approach combines first-order logic and probabilistic graphical model. First-order logic enables compactly representing the neighborhood of feature points. Probabilistic graphical model can reveal latent edges by proper inference method, and also handle the uncertainty.  

Secondly, transfer information module is tailored to handle unlabeled data. Transfer information module is realized by Self-taught Clustering algorithm [21]. Self-taught Clustering algorithm is a transfer learning method [22-26] which is built for enhancing model through large amount of auxiliary unlabeled data. The input face can be considered auxiliary unlabeled data, and find co-cluster between prior faces in the dataset. The distribution of co-cluster is further utilized to infer the possible rotation angle for robot arm, and robot arm will rotate target object from input face to output face. Finally, the validation module is an eye-to-hand camera which used to validate the error between the output face and desired target. Then, the validation module returns the error to the model in order to refine the existed model. Through these three modules, proposed system can automatically learn the relations between input images and corresponding rotation angles with only labeled the target face of each object.

In this paper, we start with briefly overview of system design and structure in section 2. The MLN-based descriptor for recognizing object is described in section 3. Section 4 introduces how to model the proposed hierarchical networks, and learn by self-taught learning. Then, we compare the performance of proposed system with several states of development in section 5. Finally, reviewing performance and conclusions are presented in the final section.


%\begin{figure}
%\begin{center}
%\subfigure[An example of an individual figure sub-caption.]{
%\resizebox*{6cm}{!}{\includegraphics{j_img/fig1.jpg}}}
%\subfigure[An example of an individual figure sub-caption.]{
%\resizebox*{6cm}{!}{\includegraphics{j_img/fig3.jp}}}
%\caption{\label{fig2} Example of a two-part figure with individual 
%sub-captions showing that captions are flush left and justified if 
%greater than one line of text, otherwise centred under the figure.}
%\label{sample-figure}
%\end{center}
%\end{figure}
\begin{figure}
\begin{center}
%\subfigure[System architecture]{}
\resizebox*{10cm}{!}{\includegraphics{j_img/fig1.jpg}}
\caption{System architecture}

\end{center}
\end{figure}
\begin{figure}
\begin{center}
\resizebox*{14cm}{!}{\includegraphics{j_img/fig3.jpg}}
\caption{Hierarchical-deep model for self-taught system}
\end{center}
\end{figure}
\section{System architecture}
The main purpose of this system is to automatically derive the relationship between input face and corresponding rotation angle to make robot arm can rotate objects to the assigned faces. The only prior knowledge are the target face. Input is arbitrary assigned object with random face on top, so input is very likely an unknown face of assigned object showed-up rather than prior target face. Therefore, system has to infer the correlation between input and existed priors.  Proposed system is shown in Fig. 1. Camera 1 captures images of all input objects with random faces on top, and constructs MLN-based descriptor for each input. Then, system matches the input with data in database and output rotation angle for robot arm. After robot arm placing an object, camera 2 will validate result, and feedback error for refining existed model. The system architecture in Fig. 1 is realized by a hierarchical-deep model in Fig. 2. 

The variables in the same layer are independent, and vertical adjacent two layers are full connected. Variables in \textbf{Feature($\bf{\Gamma}$)} layer are extracted image features, and variables in both \textbf{Classified Descriptor($\bf{D^C}$)} and \textbf{Unclassified Descriptor($\bf{D^U}$)} are MLN-based descriptor. Variables in \textbf{Rotation angle($\bf{\Theta^{C}}$)} and \textbf{Inferred Rotation angle($\bf{\Theta^U}$)} are set of rotation angles $\{$ Row ($\alpha$), Pitch($\beta$) , Yaw($\gamma$) $\}$ respect to target faces. Finally, variables in \textbf{Object($\bf{O^C}$)} are combinations of rotation angles.


The difference between classic \textbf{Deep Belief Networks}(\textbf{DBN}) is that proposed model exist two parallel parts in Fig. 2. \textbf{$\bf{D^C}$-$\bf{\Theta^{C}}$} and \textbf{$\bf{D^U}$-$\bf{\Theta^U}$} have no connection between each other, but both have full connection with deepest layer $\bf{O^C}$ and first layer \textbf{$\bf\Gamma$}. To handle tons of unknown data, the structure of connection will dynamically change with observed evidences. Sparse coding method is used to constructs edge in the model, most of connection is zero which is called latent edge in this paper. Latent edge might become non-zero while some new evidences have been discovered. For variable $d^C_w$ in layer $\bf{D^C}$, the sparse coding result should be formulated as:

\begin{equation}
d^C_w=\sum_{i\in{d^C_w}}a_{i}\Gamma_{i}+\sum_{j\notin{d^C_w}}b_{j}\Gamma_{j}
\end{equation}

Although Eq.(1) can handle the problem of latent edge, it's impractical to sample all possible conditions whenever new evidence showing up. Therefore, proposed model separate descriptor layer into two parallel parts as: 

\begin{equation}
d^C_w=\sum_{\Gamma_{i}\in{d^C_w}}a_{i}\Gamma_{i}
\end{equation}
\begin{equation}
d^U_r=\sum_{\Gamma_{j}\in{d^U_r}}b_{j}\Gamma_{j}
\end{equation}

$d^C_w$ is a prior descriptor which the edge between $\bf{\Theta^{C}}$ and $\bf{O^C}$ had been established. $d^U_r$ is a descriptor in $\bf{D^U}$ layer which match none of descriptors in $\bf{D^C}$. Therefore, we propose an inference method to infer the possible rotation angle, and camera 2 will check inferred results. If inference is success, latent edge between $d^U_r$ and $\theta^{U}$ will be established, and become stronger as more successful inferences.

\begin{equation}
d^C_w=\sum_{i\in{d^C_w}}a_{i}\Gamma_{i}+\sum_{j\notin{d^C_w}}b_{j}\Gamma_{j}
\end{equation}

Although Eq.(1) can handle the problem of latent edge, it's impractical to sample all possible conditions whenever new evidence showing up. Therefore, proposed model separate descriptor layer into two parallel parts as: 

\begin{equation}
d^C_w=\sum_{\Gamma_{i}\in{d^C_w}}a_{i}\Gamma_{i}
\end{equation}
\begin{equation}
d^U_r=\sum_{\Gamma_{j}\in{d^U_r}}b_{j}\Gamma_{j}
\end{equation}

$d^C_w$ is a prior descriptor which the edge between $\bf{\Theta^{C}}$ and $\bf{O^C}$ had been established. $d^U_r$ is a descriptor in $\bf{D^U}$ layer which match none of descriptors in $\bf{D^C}$. Therefore, we propose an inference method to infer the possible rotation angle, and camera 2 will check inferred results. If inference is success, latent edge between $d^U_r$ and $\theta^{U}$ will be established, and become stronger as more successful inferences.

\begin{figure}
\begin{center}
\subfigure[Result of background subtracton and clustering]{\resizebox*{5cm}{!}{\includegraphics{j_img/fig2b.jpg}}}
\hspace{2cm}
\subfigure[Serial captured frames]{\resizebox*{6cm}{!}{\includegraphics{j_img/fig2a.jpg}}}
\caption{Preprocessing of input objects}
\end{center}
\end{figure}


%\subfigure[An example of an individual figure sub-caption.]{
%\resizebox*{6cm}{!}{\includegraphics{j_img/fig1.jpg}}}

\section{MLN-based Descriptor}
\subsection{The concept of constructing MLN-based descriptor}

Being a self-taught system, deriving more valuable information from raw data helps system deriving more reliable results with scarce prior knowledge. Most of present image descriptors [12-16] are constructed based on strong sparse feature point, because these points are consistent even in different environment. These kinds of descriptor can efficiently and precisely match given image. Nevertheless, most of observed face is not in prior data, so we need a descriptor which can infer the relation between observations and priors. To avoid losing information, we choose normal distributed feature instead of sparse feature. Since different faces of an object may derive different strong features, normal distributed feature is more suitable for our case. 

For prepossessing of input images, each channel of RGB domain is classified into 5 parts, and get 125 classes in RGB domain. Every input image is segmented by these classes. In Fig. 3(a), an observed face of input object is segmented into 4 classes, and class 0 is background. Hereafter, predicates for MLN networks are constructed by segmented results. We have only two kinds of predicate \textit{ne(a,v)} and  \textit{des(x)} for MLN model. Variable \textit{a} is an atom cluster, and variable \textit{v} is a neighbor of atom cluster, so predicate \textit{ne(a,v)} represent adjacency of atom cluster. Variable \textit{x} in \textit{des(x)} is a MLN-based descriptor. The variables of \textbf{$\bf\Gamma$} layer in Fig. 2 are predicates \textit{ne(a,v)}. Since every class can be the atom cluster, we have $\binom{125}{2}$ binary variables in \textbf{$\bf\Gamma$} layer.

Taking Fig. 3(a) as an example, the predicates of preprocessed image are shown in Table \uppercase\expandafter{\romannumeral1}, and first order logic is formulated as:

\begin{equation}
\forall{a}\forall{v}\quad\textit{ne(a,v)}\Rightarrow\textit{des(x)}
\end{equation}

Each image will further be down sampled, and derived several images with different scales. For each image, we derive  \textbf{F*S} formulas where F is number of serial captured images and S is number of images with different scales. Through these formulas, a MLN model can be constructed. The probability distribution over possible world $d^{in}$ specified by MLN is given by:

\begin{displaymath}
P(\mathbf{D^{in}} = d^{in}) = \frac{1}{Z}exp(\sum_{j=1}^{F*S} w_j n_j(d^{in}))
\end{displaymath}
\begin{equation}
Z=\sum_{d^{in}\in\bf{D^{in}}}exp(\sum_{j=1}^{F*S} w_j n_j(d^{in}))
\end{equation}

Where $d^{in}$ is the descriptor of input image. $n_j(d^{in})$ is the number of true grounding of formula j in $d^{in}$, and $w_j$ is weight of formula j .

Consequently, probability distribution Eq.(5) is MLN-based descriptor for each 2D faces within input 3D object. 

\begin{table}
\tbl{Example of predicates}
{\begin{tabular}{@{}|c|c|c|c|c|}
\hline
Key Atom & 1 & 2 & 3 & 4\\
\hline\hline
\multirow{4}{*}{Predicates} & 
ne(1,2) & 
ne(2,1) & 
ne(3,1) &
ne(4,1)\\
\cline{2-5}
 &
ne(1,3) &
ne(2,3) &
ne(3,2) &
ne(4,2) \\
\cline{2-5}
 &
ne(1,4) &
ne(2,4) &
 &
 \\
\cline{2-5}
 &
ne(1,0) &
 &
 &
 \\
\hline
\end{tabular}}
\end{table} 

\subsection{Inference and Weight learning of MLN-based descriptor}
The weights of MLN-based descriptor are learned by maximizing the pseudo-log likelihood. Since each descriptor can be consider as a closed world, we only need to consider the atoms which derive from captured serial frames. Comparing with uniform sampling approach, maximizing pseudo-log-likelihood is more efficient, because pseudo-log likelihood only need to consider relational data. The pseudo-log likelihood of Eq.(5) can be written as:

\begin{equation}
\log P^*_w(\mathbf{D^{in}} = d^{in}) = \sum_{j=1}^{F*S} \log P_{w}(\mathbf{D^{in}} = d^{in}|\mathbf{MB}(d^{in}))
\end{equation}

Where $\bf{MB}(d^{in})$ is Markov blanket while $d^{in}$ is observed. The MLN weights are learned generatively by maximizing the pseudo-log likelihood of Markov blanket. The gradient of the pseudo-log likelihood with respect to the weight is:

\begin{displaymath}
\begin{array}{ll}
\dfrac{\partial}{\partial w_i}\log P^*_w(\mathbf{D^{in}} = d^{in})= &\\\\
\sum^{F*S}_{j=1}\{n_i(d^{in})-P_w(\mathbf{D^{in}}=0|\mathbf{MB}(d^{in}))n_i(d^{in}=0) &\\
\end{array}
\end{displaymath}
\begin{equation}
-P_w(\mathbf{D^{in}}=1|\mathbf{MB}(d^{in})n_i(d^{in}=1)\}
\end{equation}

Where $n_i(d^{in}=0)$ is the number of true grounding of $j^{th}$ formula while force $\mathbf{d^{in}}=0$, and similar for $n_i(d^{in}=1)$. The learning of pseudo-log-likelihood in our approach are further boosted by  \textbf{\textit{Limited-memory Broyden-Fletcher-Goldfarb-Shanno(L-BFGS)}} optimizer [20] to make entire process become more efficiency.

\subsection{Matching of MLN-based descriptors}
For each constructed input descriptor $d^{in}_k$, system would search for the matched descriptor in the database, and further arrange it to the proper layer of $\bf{D^C}$ or $\bf{D^U}$ as shown in Fig.2. Since input is possible to be assigned to one of parallel layers, matching step is separated into two parts. One is using pseudo-log likelihood for deciding observation should be assigned to which layer. The pseudo-log likelihood of descriptors matching could be formulated as:


\begin{equation}
\begin{array}{ll}

\mathbf{argMax}P(\mathbf{D^C} = d^C_w  | \mathbf{D^{in}} = d^{in} , \mathbf{\Gamma}_{k \in {d^{in}}}) &\\\\
= \mathbf{argMax}P(d^{in} | \mathbf{MB}(d^{in}){P(d^{C}_w | \mathbf{MB}(d^{in}))}
\end{array}
\end{equation}


If input descriptors match none of descriptor in $\bf{D^C}$ layer, the descriptor become a variable of  $\bf{D^U}$ layer. For a variable in $\bf{D^U}$, we infer rotation angle to make input object which can be placed on corresponding target face. Since the rotation angles for descriptors in $\bf{D^U}$ are unidentified, the second part for matching is trying to find a descriptor in $\bf{D^C}$ which have max co-cluster with input descriptor. Finding max co-cluster can be alternately considered as minimizing information loss as:
\begin{equation}
\mathbf{argMin}(I(d^in , \mathbf{\Gamma}_{k \in d^{in} \cap d^C_w}) - I(d^C_w , \mathbf{\Gamma}_{k \in d^{in} \cap d^C_w}))
\end{equation}

The common feature $\mathbf{\Gamma}_{k \in d^{in} \cap d^C_w}$ is further represented by Markov Blanket of $d^{in}$ and $d^C_w$, and the loss of mutual information can be further formulated by \textbf{\textit{Kullback–Leibler divergence(KL divergence)}}[28] as:

\begin{equation}
\begin{array}{ll}
\mathbf{argMin}D(P(d^{in} , \mathbf{MB}(d^{in},d^c_w))||P(d^c_w , \mathbf{MB}(d^{in},d^c_w)))\\\\
= \mathbf{argMin}\sum_{\Gamma_k\in\mathbf{MB}(d^{in},d^c_w)}P(\Gamma_k)D(P(d^{in}|\Gamma_k)||P(d^C_w|\Gamma_k))\\\\
\end{array}
\end{equation}

In Eq.(10), classified descriptor $d^C_w$ with min KL divergence is considered as acquired max co-cluster with $d^{in}$. The relation between the co-cluster become the evidence for inferring rotation angle of $d^{in}$. Through Eq.(8) and Eq.(10), the input descriptors are classified to corresponding layer, and become inputs $\bf{\Theta^{U}}$ or $\bf{\Theta^{C}}$ layer.


\section{Hierarchical model}
\subsection{Inference of rotation angle in $\bf{\Theta^{U}}$ layer}
Inference rotation angle $\bf{\theta^{U}_i}$ is based on max co-cluster between $d^{in}$ and $d^C_w$. A set of co-cluster $\{$$C_{w1}$, $C_{w2}$, ..., $C_{wL}$$\}$ can be derived by minimizing KL divergence. The center of co-cluster with respect to center of camera in Cartesian space can be derived into two sets $\bf{V^{in}}$=$\{$$v^{in}_1$, $v^{in}_2$, ..., $v^{in}_L$ $\}$ and $\bf{V^{C}_w}$=$\{$$v^{C}_{w1}$, $v^{C}_{w2}$, ..., $v^{C}_{wL}$ $\}$. $\bf{V^{in}}$ is a set of co-cluster position in input descriptor, and $\bf{V^{C}_w}$ is a set of co-cluster position in $d^C_w$. The roll angle $\alpha$ of robot arm is calculated by:

\begin{equation}
\alpha =\cos^{-1}\frac{1}{L}\sum_{l=1}^K\frac{v^C_{wl}-v^{in}_l}{|v^C_{wl}-v^{in}_l|}
\end{equation}

Where roll angle $\alpha$ is the mean angle of co-cluster in two descriptors. As for  pitch angle $\beta$ and yaw angle $\gamma$, the pitch and yaw angle are hard to be estimated by 2D descriptor directly. We make random sample these two angles in value $\pi/2$ , and $-\pi/2$ initially, and approximate to actual angles by algorithm 1.



\begin{algorithm}[!t]
  \caption{Inferring rotation angle from co-cluster}
   $\bf{Function}$ inferringTheta$(d^{in},\bf{D^C},\bf{D^U})$\\ 
  $\bf{Input:}$\\
   $d^{in}$, input descriptor\\
   $\bf{D^C}$, descriptors in $\bf{D^C}$ layer\\
   $\bf{D^U}$, descriptors in $\bf{D^U}$ layer\\
  $\bf{Output:}$\\
  $\theta^U\{\alpha,\beta,\gamma\}$, rotation angle for robot arm\\        
  \begin{algorithmic}[1]
    \State $L\_{D^c} \leftarrow$ maxLikelihood($d^{in}$, $\bf{D^C}$)
    \State $L\_{D^U} \leftarrow$ maxLikelihood($d^{in}$, $\bf{D^U}$)
    \If{$L\_{D^U} > L\_{D^U}$}
    
 	$d\_target \leftarrow $ maxLikelihood($d^{in}$, $\bf{MB}$ in $\bf{D^U}$)
 		
 	$\theta^U\leftarrow$findmax$\_$Coclass($d^{in}$, $d\_target$)
 	
 		\While{t $<$ max$\_$t $||$ maxLikelihood(t)$>$ threshold}
 		
 			\If{maxLikelihood(t) $>$ maxLikelihood(t-1)}
 			
 				$\;\;\;\;\;\;t++$
 				
 				$\;\;\;\;\;\;\theta^U \leftarrow \theta^U+Step$
 				
 			\Else
 			
 				$\;\;\;\;\;\;Break$
 			\EndIf
 		
 		
 		
    	\EndWhile
    \Else
    
    	$d\_target \leftarrow $ maxLikelihood($d^{in}$, $\bf{MB}$ in $\bf{D^C}$)
    	
    	$\theta^U\leftarrow$findmax$\_$Coclass($d^{in}$, $d\_target$)
    \EndIf
    
	\State $\bf{Return}$ $\theta^U\{\alpha,\beta,\gamma\}$
    
  \end{algorithmic}
\end{algorithm}

\subsection{Inference and learning of hierarchical-deep model}

\begin{figure}[!t]
\begin{center}
\resizebox*{6cm}{!}{\includegraphics{{j_img/fig4.jpg}}}
\caption{Structure configuration of each layer in the proposed model}
\end{center}
\end{figure}

Proposed hierarchical model is a generative model of \textbf{\textit{Deep Belief Network (DBN)}}. Structure between each layer is shown in Fig. 4. each layer is considered as a \textbf{\textit{Restricted Boltzmann Machine (RBM)}}[8] except $\bf{\Gamma}$, $\bf{D^C}$, and $\bf{D^U}$. The MLN is trained by pseudo-log likelihood as mentioned previously, and RBM is trained by greedy layer-wise training [30].

Initially, left part of model ($\bf{\Gamma}$-$\bf{D^C}$-$\bf{\Theta^C}$-$\bf{O^C}$) are trained with prior target face of objects, and number of variables $\bf{N}$ in $\bf{O^C}$ equal to the number of prior target faces. The right part of model is activated only when a new observation is classified into $\bf{D^U}$. The activation probability of $\theta^U_i$ is a sigmoid activation function:

\begin{equation}
\begin{array}{ll}
P(\theta^U_i|\bf{MB}(d^{in}))=\frac{1}{1+exp \left( \mu*b-\sum_r d^U_r w_{r}\right)} \\\\
\mu = \left\{
        \begin{array}{ll}
         0  & ,\mbox{if inference succeed} \\
         1+logP(\theta^U_i|\bf{\Theta^U}) & ,\mbox{if inference fail} \\ 
		\end{array}
	   \right.
\end{array}
\end{equation}



where $w_r$ and $b$ are the weights and bias. $\mu$ is penalty factor which decreases the probability while the inference is failed. $\mu$ is depended on log likelihood of $\theta^U_i$ which can lead to lower activation probability if inference result failed several times, and avoid system derives wrong results over again. 
On the other hand, for both $\bf{\Theta^U}$ and $\bf{\Theta^C}$ layer, if results are correct, the model will be retrained by greedy layer-wise training. If validated result is derived from left part of Fig. 4, the generative model is defined by the joint distribution of top layers $P(\bf{O^C} , \bf{\Theta^C})$. If the result is derived from right part, the generative model is defined by  $P(\bf{O^C} , \bf{\Theta^U})$. By doing so, the relation between prior and observations can be self-taught from numerous random unlabeled inputs, and infer possible relationship without any manual intervention.


\section{Experiments}
The experiments for proposed model are separated into two parts. Firstly, we evaluate the performance of MLN-based descriptor by standard object recognition datasets: $\bf{Caltech-101}$ [31] and $\bf{Caltech-256}$ [32]. Results shown in Table \uppercase\expandafter{\romannumeral2} are comparisons with recently published papers. The images in the datasets are rescaled into five different scales for training proposed MLN-based descriptor. For $\bf{Caltech-101}$, we follow general procedure and randomly selecting 30 images for each class. For $\bf{Caltech-256}$, select 45 and 60 images for each class, and trained by pseudo-log likelihood. Although, for the proposed model, the result does not outperform in $\bf{Caltech-101}$, but the accuracy in $\bf{Caltech-256}$ is slightly behind ImageNet-pretrained model. In the other scopes, the result shows that MLN-based descriptor keep well performance even increasing categories. Most of recently published methods get dramatically performance decreasing while categories increase from 101 to 256. 


\begin{table}
\tbl{Comparisons on $\bf{Caltech-101}$}
{\begin{tabular}{|c|c|}
\hline
Methods & Accuracy \\
\hline
$\bf{MLN-based}$ & $\bf{74.6}$ \\
\hline
LLC[37] & 73.1 \\
\hline
P-LLC[38] & 78.75\\
\hline
P-FV[38] & 80.1\\
\hline
M-HMP[39] & 82.5 \\
\hline
ImageNet-pretrained convnet[40] & 86.5 \\
\hline
\end{tabular}}
\end{table}

\begin{table}
\tbl{Comparisons on $\bf{Caltech-256}$}
{\begin{tabular}{|c|c|c|}
\hline
Methods & 45 & 60 \\
\hline
$\bf{MLN-based}$ & $\bf{66.7}$ & $\bf{69.6}$ \\
\hline
LLC &  45.3 & 47.7\\
\hline
P-LLC & 44.9 & 48.0\\
\hline
P-FV & 44.9 & 52.6\\
\hline
M-HMP & 54.8 & 58 \\
\hline
ImageNet-pretrained convnet & 72.7 & 74.2 \\
\hline
\end{tabular}}
\end{table}



\begin{table}
\tbl{Three classes of testing work pieces for experiments}
{\resizebox*{14cm}{!}{\includegraphics{{j_table/wptype.png}}}}
\end{table}



For the second part of experiment, we implement the proposed system in real industrial application. The prior knowledge is target faces of assigned objects, and there are twenty kinds of assigned objects in our experiment. Table \uppercase\expandafter{\romannumeral4} shows twenty target faces for each assigned object. The experiment is implemented based on several assumptions: The input objects are not occluded, and not adjacent with each other. Hereafter, the inputs are randomly chosen from assigned objects with random face on top. 

The testing objects are classified into three classes in Table \uppercase\expandafter{\romannumeral4}. For class $\bf{WP1}$, the work pieces are featureless and relative small, so it's hard to construct robustness descriptor even building relational model for entire model. For class $\bf{WP2}$, all work pieces acquire similar shapes and size, so this kind of object is easily mismatched in the matching process. The work pieces in $\bf{WP3}$ are matched group of this experiment. These work pieces acquire sufficient features for descriptor, and have plenty of information for identifying and constructing descriptor. In the first stage of experiment, we compare the performance of proposed system between different classes in different lighting conditions.  The results of different classes are shown in Fig.5. In Fig.5(a), the environment lighting is controlled by on-axis lighting source, so the information of object are more complete and distinct than images without lighting control in Fig.5(b). The accuracy is average of 100 times repeatedly testing.

The system is considered convergence while accuracy is over 95$\%$, and stop learning approach. If the accuracy is under 95$\%$ again, the learning approach would be re-excuted. Comparing the results, in both cases, class $\bf{WP3}$ could be convergent with least input sample, and convergent time of class $\bf{WP2}$ is slowest. The results show that the efficiency of learning could be slightly improved by environment constrain, but the accuracy is not effected, and always keep over 95$\%$ after learning approach stopped. 
 
Fig. 6 shows the result while all twenty kinds of assigned object are involved in the same time. The result shows that system need more inputs to converge while more kinds of objects are involved, but the system still slightly converge, and accuracy is all keeping over 95$\%$ for both conditions. In brief, these two experiments verify proposed system is competent to learn the HD model automatically. Although the learning rate is dragged by the number of assigned objects, the learning rate still can be convergent by reasonable number of inputs. 

\begin{figure}
\begin{center}
	\subfigure[Performance with on-axis lighting source]{
		\resizebox*{7cm}{!}{\includegraphics[width=3 in]{j_img/exp01.png}}}

	\subfigure[Performance without on-axis lighting source]{
		\resizebox*{7cm}{!}{\includegraphics[width=3 in]{j_img/exp02.png}
	}}

\caption{Experimental Results of different classes in different lighting conditions}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
	\subfigure[Performance with on-axis lighting source]{
		\resizebox*{7cm}{!}{\includegraphics[width=3 in]{j_img/exp03.png}
	}}

	\subfigure[Performance without on-axis lighting source]{
		\resizebox*{7cm}{!}{\includegraphics[width=3 in]{j_img/exp04.png}
	}}

\caption{Experimental Results of all work pieces in different lighting conditions}
\end{center}
\end{figure}

The experiment in Fig. 5 and 6 testified the performance of proposed system can meet our requirements. We compare the performance of proposed system with other advanced approaches. Since none of similar systems could handle this issue in our literature survey results, so the comparisons are done by dividing our system into two parts. One is 2D descriptors for each face of objects, and the other is machine learning approach for learning relational model.

For the descriptor part, four kinds of other descriptors are chosen to compare with proposed system. B-SIFT[35]and Edge-SIFT[36] are modified versions of SIFT approach which enhanced the accuracy of feature point registration.  \textbf{\textit{Binary Robust Invariant Scalable Keypoints(BRISK)}}[15] descriptor is constructed based on binary robust invariant scalable key points, and \textbf{\textit{Zernike Moment (ZM)}}[13] phase-based descriptor is a moment-based descriptor which use the phase information of signal. All of these descriptors are representative methods in relative field recent years, and had been testified by many researchers. To compare the robustness and accuracy, the performance is testified by two conditions as shown in Table \uppercase\expandafter{\romannumeral5}. One is relationship of each face is prior of system, and the descriptors only provide information for object matching. The experiments are implemented by the same learning approach which is proposed in the previous section. The other is no prior for learning approach that information of descriptor need to be used for inferring the relational model. The ZM descriptor has the best performance in the condition with prior, but results of descriptors are close. In condition, without prior, the MLN-based descriptor acquires best performance which testified MLN-based descriptor is suited for handling large amount of unlabeled data. 

Hereafter, the performances of different learning methods are further discussed. The other three kinds of transfer learning approaches: \textbf{\textit{Locally Weighted Ensemble approach(LWE)}}[25], \textbf{\textit{Transductive SVM(TSVM)}}[26], and self-taught learning[21] are chosen to compare with proposed method. Similarly, the experiments are divided into two parts as shown in Table \uppercase\expandafter{\romannumeral6}. The result shows LEW acquires the best accuracy in the condition with priors, and proposed learning approach acquire greatest performance in condition without prior. Although the performances between different methods are quite close when priors are provided, the accuracy of the other methods goes down in no prior condition. It seems that the results are not only affected by descriptor, but also learning approach. The proposed system is only one method which can automatically learn and recognize object without prior knowledge of 3D model. 

\section{Comparisons of related works}
The proposed system is a HD model with parallel branches. Conventionally, the descriptors [13, 15, 35-39] and learning model [27-28] are separated, so that the learning approaches only learn the distribution of descriptors but cannot adjust the distribution of descriptors. According to our surveys, there are no such descriptors construction can fit every case without manual intervention. Therefore, we propose to establish a learning approach which can adjust the distribution of descriptors to make the MLN-based descriptor can be refined during learning stage. This approach is proved by experimental results as shown in Table V and VI. The descriptors have to be constructed without manual intervention under the condition of without prior knowledge. 

Moreover, the other distinct feature of our proposed system is that the learning knowledge in both image and rotation angle can be done by just only one model. Most of HD models [8-11] are focus on learning knowledge in the same domains, e.g.  handwriting [8], text categorization[9], Speech Recognition [10], images[11]. To handle the knowledge in different domains, we involve the technique of self-taught clustering and parallel structure model, and the experimental results also support the feasibility of learning knowledge in different domains by our proposed HD model.

\begin{table}
\tbl{Comparisons of different descriptors}
{
\resizebox*{8cm}{!}{\includegraphics{{j_table/table5.png}}}
}
\end{table}

\begin{table}
\tbl{Comparisons of different learning approaches}
{
\resizebox*{8cm}{!}{\includegraphics{{j_table/table6.png}}}
}
\end{table}

\section{Conclusion}
An automatic learning system for vision system is an important part in assembling production line with small-volume, large-variety components. In this work, we reverse the concept of traditional vision system. The robustness of feature points and descriptor is not main concerns. Instead, the relation between input and output is the most essential. 

To learn the relationship between input and output, we propose a HD model which combines the concept of deep learning, transfer learning, and Markov logic network. The model acquires self-taught ability which can infer relational model and self-supervised the performance of learning results. Being an automatic system, tackling large amount of unlabeled data and inferring relation with labeled data is necessary. The relation between features in different levels can be represented as a discriminative distribution by the model. Through the discriminative distributions, KL divergence is further involved to find the max co-cluster between labeled and unlabeled data, and makes system stable under growing database. 

Moreover, proposed system includes image features, descriptors and rotation angles for robot arm. These different domain features are impossible to be learned simultaneously by traditional single layer model, but the experimental results prove proposed HD model can transfer and learn different domain knowledge, and recognize 3D object without manual intervention. We believe this system is practical in real industrial assembling production line, and save labor cost.



\begin{thebibliography}{12}
\bibitem{c01} Torgny Brogrdh, ``Present and future robot control development - An industrial perspective,'' \textit{Annual Reviews in Control}, Vol. 31, no. 1, pp. 69-79, 2007.
\bibitem{c02} Ebrahim Mattar, ``Robotics Arm Visual Servo: Estimation of Arm-Space Kinematics Relations with Epipolar Geometry, Robotic Systems - Applications, Control and Programming,"\textit{Dr. Ashish Dutta (Ed.)}, ISBN: 978-953-307-941-7, InTech, DOI: 10.5772/25605.
\bibitem{c03} So-Youn Park ,Yeoun-Jae Kim ,Ju-Jang Lee ,Byung Soo Kim, and Khalid A.Alsaif, ``Controlling robot arm manipulator using image-based visual servoing without pre-depth information,'' in \textit{Proc. IEEE Int. Conf. Ind. Ele.}, 2011, pp. 3157-3161.
\bibitem{c04} K. Deguchi, H. Sakurai, and S. Ushida, ``A Goal Oriented just-in-time visual servoing for ball catching robot arm,'' in \textit{Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst.}, 2008, pp. 3034-3039.
\bibitem{c05} J. Baker, L. Deng, J. Glass, S. Khudanpur, Chin hui Lee, N. Morgan, and D. OShaughnessy, ``Developments and directions in speech recognition and understanding, part 1,'' \textit{IEEE Signal Processing Magazine}, vol. 26, no. 3, pp. 75-80, May. 2009.
\bibitem{c06} S. Furui, `Digital Speech Processing, Synthesis,'' in \textit{Marcel Dekker}, 2000.
\bibitem{c07} Tong Simon, and Daphne Koller, ``Support vector machine active learning with applications to text classification,'' \textit{The Journal of Machine Learning Research}, pp. 45-66, 2002.
\bibitem{c08} Hinton, G. E., Osindero, S. and Teh, Y, ``A fast learning algorithm for deep belief nets,'' \textit{Neural Computation}, pp. 1527-1554, 2006.
\bibitem{c09} Srivastava, N., Salakhutdinov, R. R. and Hinton, G. E., ``Modeling Documents with a Deep Boltzmann Machine,'' in \textit{Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing}, 2013.
\bibitem{c10} Graves, A., Mohamed, A. and Hinton, G. E., ``Speech Recognition with Deep Recurrent Neural Networks,'' in \textit{ Proc. Int. Conf. Acoustics, Speech and Signal Processing}, 2013, pp. 6645-6649. 
\bibitem{c11} Ranzato, M., Mnih, V., Susskind, J. and Hinton, G. E., ``Modeling Natural Images Using Gated MRFs,'' \textit{IEEE Trans. Pattern Analysis
and Machine Intelligence}, 2013.
\bibitem{c12} H. Bay, A. Ess, T. Tuytelaars, and L. Gool, ``SURF: Speeded up robust features,'' \textit{Comput. Vis. Image Understand.},  vol. 110, no. 3, pp. 346-359, Mar. 2008.
\bibitem{c13} Zen Chen and Shu-Kuo Sun, ``A Zernike Moment Phase-Based Descriptor for Local Image Representation and Matching,'' \textit{IEEE Trans. Image Process}, vol. 19, no. 1,pp. 205219, Jan. 2010.
\bibitem{c14} A. Alahi, R. Ortiz, and P. Vandergheynst, ``Freak: Fast retina keypoint,'' in \textit{Proc. Computer Vision and Pattern Recognition}, 2012.
\bibitem{c15} S. Leutenegger, M. Chli, and R. Siegwart, ``Brisk: Binary Robust Invariant Scalable Keypoints,'' in \textit{Proc. IEEE Int. Conf. Computer Vision}, 2011.
\bibitem{c16} Vijay Chandrasekhar, Gabriel Takacs, David Chen, Sam S. Tsai, Jatinder Singh, and Bernd Girod, ``Transform coding of image feature descriptors,'' \textit{Visual Communications and Image Processing}, 2009.
\bibitem{c17} Matthew Richardson and Pedro Domingos, ``Markov logic networks,'' \textit{Journal of Machine Learning}, Vol. 62, no. 1, pp. 107-136, Feb. 2006.
\bibitem{c18} L. Mihalkova, T. Huynh, and R.J. Mooney, ``Mapping and Revising Markov Logic Networks for Transfer Learning,'' in \textit{Proc. Conf. Ass. Adv. Arti. Int.}, 2007, pp. 608-614.
\bibitem{c19} Kok, Stanley and Domingos, Pedro, ``Learning the Structure of Markov Logic Networks,'' in \textit{Proc. Int. Conf. Machine Learning}, 2005, pp. 441-448.
\bibitem{c20} Parag Singla and Pedro Domingos, ``Discriminative training of Markov logic networks,'' in \textit{Proc. Int. Conf. Artiﬁcial Intelligence}, 2005.
\bibitem{c21} Wenyuan Dai, Qiang Yang, Gui-Rong Xue and Yong Yu, ``Self-taught Clustering,''  in \textit{Proc. Int. Conf. Machine Learning}, 2008.
\bibitem{c22} Saˇso Dˇzeroski, ``Multi-relational Data Mining: An Introduction," \textit{SIGKDD Explore Newsletter}, Vol. 5, No. 1, Jul. 2003, pp.1-16.
\bibitem{c23} Sinno Jialin Pan, and Qiang Yang, ``A Survey on Transfer Learning,'' \textit{IEEE Transactions on Knowledge and Data Engineering}, vol. 22, no. 10, pp.1345-1359, Oct. 2010.
\bibitem{c24} T. Dietterich, L. Getoor, and K. Murphy, ``Statistical Relational Learning and its Connections to Other Fields,'' in \textit{Proc. Int. Conf. Machine Learning}, 2004.
\bibitem{c25} Jing Gao and Wei Fan and Jing Jiang and Jiawei Han, `Knowledge Transfer via Multiple Model Local Structure Mapping,'' in \textit{Proc. Int. Conf. Knowledge Discovery and Data Mining}, 2008, pp. 283-291.
\bibitem{c26} T. Joachims, ``Making large-scale svm learning practical.”, advances in kernel methods - support vector learning,'' MIT-Press, 1999.
\bibitem{c27} A. J. Carlson, C. M. Cumby, J. L. R. Nicholas D. Rizzolo, and D. Roth, ``Snow learning architecture,'' \textit{Technical report UIUCDCS}, 1999.
\bibitem{c28} Cover, T. M. and Thomas, J. A., ``Elements of information theory,'' Wiley-Interscience.
\bibitem{c29} Yoshua Bengio, Pascal Lamblin, Popovici Dan, and Larochelle Hugo, ``Greedy Layer-Wise Training of Deep Networks,'' \textit{Advances in
Neural Information Processing Systems}, 2007.
\bibitem{c30} Fei-Fei L., R. Fergus, and P. Perona, ``Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories,'' in \textit{Proc. Computer Vision and Pattern Recognition}, 2004, pp. 178-178.
\bibitem{c31} Grifﬁn, G. Holub, and P. Perona, ``The Caltech 256,'' \textit{Caltech TechnicalReport}.
\bibitem{c32} Karthikeyan Vaiapury, Anil Aksay and Ebroul Izquierdo, ``GrabcutD: Improved Grabcut Using Depth Information,'' in \textit{Proc. Int. Conf. ACM}, 2008, pp. 57-62.
\bibitem{c33} Z. Zivkovic, ``GrabcutD: Improved Grabcut Using Depth Information,'' in \textit{Proc. Int. Conf. Pattern Recognition}, 2004, pp. 28-31.
\bibitem{c34} Fei Sha and Fernando Pereira, ``Shallow parsing with conditional random ﬁelds,'' in \textit{Proc. Int. Conf. North American
Chapter of the Association for Computational Linguistics on Human Language Technology}, 2003.
\bibitem{c35} Yanning Zhang,Zhi-Hua Zhou, Changshui Zhang and Li, Ying, ``B-SIFT: A Highly Efficient Binary SIFT Descriptor for Invariant Feature Correspondence,'' in \textit{Proc. The Second Sino-foreign-interchange Conference on Intelligent Science and Intelligent Data Engineerin}, pp.426-433, 2012.
\bibitem{c36} S. Zhang, Q. Tian, K. Lu, Q. Huang and W. Gao, ``Edge-SIFT: Discriminative binary descriptor for scalable partial-duplicate mobile search,'' \textit{IEEE Transactions on Image Process}, vol. 22, no. 7, pp.2889-2902, Jul. 2013.
\bibitem{c37} J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Guo., ``Locality-constrained Linear Coding for Image Classication,'' in \textit{Int. Conf. CVPR},pp. 3360-3367, 2010.
\bibitem{c38} L. Seidenari, G. Serra, AD. Bagdanov, and A. Del Bimbo, ``Local Pyramidal Descriptors for Image Recognition,'' \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 36, no. 5,pp.1033-1040, May,2014.
\bibitem{c39} Liefeng Bo, Xiaofeng Ren, and D. Fox, ``Multipath Sparse Coding Using Hierarchical Matching Pursuit,'' in \textit{Proc. Computer Vision and Pattern Recognition}, 2013, pp. 660-667.
\bibitem{c40} Matthew D Zeiler, and Rob Fergus, ``Visualizing and Understanding Convolutional Networks,'' in \textit{Proc. ECCV}, 2014,  pp. 818-833.

\end{thebibliography}

%\bigskip
%\noindent This was produced by typing:
%\medskip
%
%\begin{verbatim}
%\begin{thebibliography}{12}
%
%\bibitem{neu83}
%Neumann M. Parallel GRASP with path-relinking for job shop scheduling. Mol
% Phys. 1983;50:841--843.
%
%\bibitem{ed84}
%Edwards DMF, McDonald IR. Positive bases in numerical optimization. Comput
% Optim Appl. 1984;21:169--175.
%
%\bibitem{aiex00}
%Aiex RM, Pierce IF, Donizetti G, von~Weber CM, Bizet G, Bach CPE, Strauss R,
% van~Beethoven L, Mozart WA, Dukas P. Computing tools for modelling orchestral
% performance. University of Cambridge, Cambridge, UK; 2000.
% Tech Rep DAMTP 2000/NA10.
%
%\bibitem{Eri1984}
%Ericsson KA, Simon HA. Protocol analysis: verbal reports as data. Cambridge
% (MA): MIT Press; 1984.
%
%\bibitem{glov00}
%Glover F. Multi-start and strategic oscillation methods -- principles to
% exploit adaptive memory. In: Laguna M, Gonz\'{a}les-Velarde JL, editors.
% Computing tools for modeling, optimization and simulation: interfaces in
% computer science and operations research. 2nd ed. Vol.~2. Boston (MA): Kluwer
% Academic; 2000. p. 1--24.
%
%\bibitem{hk97}
%Kern H. The resurgent Japanese economy and a Japan--United States free trade
% agreement. In: Lambert C, Holst G, editors. 4th International Conference on
% the Restructuring of the Economic and Political System in Japan and Europe.
% Milan, Italy, 1996 May 21--25. Singapore: World Scientific; 1997. p. 147--156.
%
%\bibitem{glov86}
%Glover F. Hilbert modular forms and the Galois representations associated to
% Hilbert--Blumenthal abelian varieties [Ph.D. thesis]. Harvard University,
% Cambridge (MA); 1986.
%
%\bibitem{Agu95}
%Agutter AJ. The linguistic significance of current British slang [unpublished
% doctoral dissertation]. Edinburgh University, UK; 1995.
%
%\bibitem{Holl04}
%Holland M. Guide to citing internet sources. 2004 [cited~2012 Nov 4].
% Available from:
% http://www.bournemouth.ac.uk/library/using/guide\_to\_citing\_internet\_sourc.html.
%
%\bibitem{Maz91}
%Mazzeo J. Comparability of computer and paper-and-pencil scores (College Board
% Rep. No. 91). Princeton (NJ): Educational Testing Service; 1991.
%
%\bibitem{Mil93}
%Miller ME. The interactive tester (version 4.0) [computer software].
%  Westminster (CA): Psytek Services; 1993.
%
%\bibitem{cwm73}
%Misner CW. Efficient algorithms for layer assignment problems. In: Gottlob I,
% editor. Gravitation in a collapsing Universe. 2nd ed. Vol.~5, Einstein's
% Legacy. San Francisco (CA): Freeman; 1973. p. 63--83.
%
%\end{thebibliography}
%\end{verbatim}




\end{document}
